import os
import zipfile
import pandas as pd
from dotenv import load_dotenv

# ImportaÃ§Ã£o relativa para os mÃ³dulos vizinhos
from ans_scrapper import ANSScraper
from data_processor import DataProcessor

load_dotenv()  # Carrega as variÃ¡veis do .env


class ANSService:
    """
    ServiÃ§o Orquestrador: Centraliza configuraÃ§Ãµes e coordena o fluxo ETL.
    """

    # Buscamos do .env. Se nÃ£o existir, usamos uma string vazia como fallback
    BASE_URL = os.getenv("ANS_DATA_SOURCE_URL", "")
    DATA_DIR = os.path.abspath(os.path.join(
        os.path.dirname(__file__), "../../../data"))


    @classmethod
    def consolidar_e_analisar(cls, lista_dfs):
        """
        Consolida os dados, trata inconsistÃªncias e gera o ZIP final (Item 1.3).
        """
        if not lista_dfs:
            print("âš ï¸ Sem dados para consolidar.")
            return

        print("\nğŸ“Š Iniciando consolidaÃ§Ã£o e anÃ¡lise de inconsistÃªncias...")

        # 1. Juntar todos os DataFrames
        df_final = pd.concat(lista_dfs, ignore_index=True)

        # --- TRATAMENTO DE INCONSISTÃŠNCIAS ---

        # A. Tratar Valores (Ignorar ou corrigir negativos/zerados)
        # DecisÃ£o tÃ©cnica: Converter para numÃ©rico e filtrar apenas > 0
        df_final['VALORDESPESAS'] = pd.to_numeric(
            df_final['VALORDESPESAS'], errors='coerce').fillna(0)
        iniciais = len(df_final)
        df_final = df_final[df_final['VALORDESPESAS'] > 0]
        print(
            f"  ğŸ§¹ Valores: Removidas {iniciais - len(df_final)} linhas com valores invÃ¡lidos ou <= 0.")

        # B. Tratar CNPJs e RazÃ£o Social (Conflitos)
        # DecisÃ£o: Manter a primeira RazÃ£o Social encontrada para cada CNPJ (PadronizaÃ§Ã£o)
        df_final = df_final.sort_values(by=['CNPJ', 'RAZAOSOCIAL'])
        df_final['RAZAOSOCIAL'] = df_final.groupby(
            'CNPJ')['RAZAOSOCIAL'].transform('first')

        # C. Remover Duplicados Reais (Mesmo CNPJ, Ano, Trimestre e Valor)
        df_final = df_final.drop_duplicates()

        # 2. Gerar o CSV Final
        csv_path = os.path.join(cls.DATA_DIR, "consolidado_despesas.csv")
        df_final.to_csv(csv_path, index=False, sep=';', encoding='utf-8-sig')

        # 3. Compactar em ZIP (conforme pedido)
        zip_final_path = os.path.join(cls.DATA_DIR, "consolidado_despesas.zip")
        with zipfile.ZipFile(zip_final_path, 'w', zipfile.ZIP_DEFLATED) as z:
            z.write(csv_path, arcname="consolidado_despesas.csv")

        print(f"âœ… Arquivo final gerado: {zip_final_path}")
        return zip_final_path

            
# Exemplo simples para teste manual
if __name__ == "__main__":
    # 1. Identificar (Passamos a URL como argumento)
    urls = ANSScraper.identificar_arquivos_trimestrais(ANSService.BASE_URL)

    if urls:
        # 2. Baixar (Passamos a pasta de destino como argumento)
        caminhos_zips = ANSScraper.baixar_arquivos(urls, ANSService.DATA_DIR)

        if caminhos_zips:
            print("\nğŸ§ª Iniciando processamento dos dados...")
            # 3. Processar (Passamos a pasta de dados para o processador saber onde criar a temp)
            lista_dataframes = DataProcessor.processar_e_normalizar(
                caminhos_zips, ANSService.DATA_DIR)

            if lista_dataframes:
                # 4. Consolidar via SERVICE (onde a funÃ§Ã£o ficou)
                ANSService.consolidar_e_analisar(lista_dataframes)
                print(
                    f"âœ… Sucesso: {len(lista_dataframes)} arquivos de despesas foram carregados na memÃ³ria.")
                # Aqui vocÃª jÃ¡ tem os dados prontos para a ConsolidaÃ§Ã£o (Item 1.3)
            else:
                print(
                    "âš ï¸ Nenhum arquivo de despesas/sinistros foi encontrado dentro dos ZIPs.")
    else:
        print("âŒ NÃ£o foi possÃ­vel encontrar os links para download.")   
